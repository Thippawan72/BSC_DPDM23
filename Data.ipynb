{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JIxmxWM05ZmRFVTG0fo6FB8utIMnEtIs",
      "authorship_tag": "ABX9TyMebVs/kc4ei1Mnc1QL+VRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thippawan72/BSC_DPDM23/blob/main/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VhBw2AmK4sGQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "jEVHz8jCFOnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install moviepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23WOv5CmOHg",
        "outputId": "e937ffc5-864c-4162-a657-4e8ca1134973"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f05dnQR43St",
        "outputId": "88d7b1ba-f10d-49ee-9ad9-706f8e766417"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "e4GtPYPDnl3F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input and output directories\n",
        "input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /การเงินและธุรกรรม'\n",
        "output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /วิดีโอการเงินและธุรกรรม'"
      ],
      "metadata": {
        "id": "dxA0Dw77n1ZC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert GIF to MP4\n",
        "def convert_gif_to_mp4(input_gif, output_mp4):\n",
        "    try:\n",
        "        # Load the GIF file\n",
        "        clip = VideoFileClip(input_gif)\n",
        "\n",
        "        # Write the video to MP4 format\n",
        "        clip.write_videofile(output_mp4, codec='libx264')\n",
        "        print(f\"Successfully converted {input_gif} to {output_mp4}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {input_gif}: {e}\" )"
      ],
      "metadata": {
        "id": "1OSXPthpm8r3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all GIFs in a directory\n",
        "def preprocess_videos(input_dir, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Loop over all files in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(\".gif\"):\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            output_filename = filename.replace(\".gif\", \".mp4\")\n",
        "            output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            # Convert GIF to MP4\n",
        "            convert_gif_to_mp4(input_path, output_path)\n",
        "\n",
        "# Start preprocessing the GIFs\n",
        "preprocess_videos(input_directory, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "HVpZAsKSna4v",
        "outputId": "4debe6ba-a1e6-4852-e912-e3588a1b9ce4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /การเงินและธุรกรรม'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fdfe1062127f>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Start preprocessing the GIFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpreprocess_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-fdfe1062127f>\u001b[0m in \u001b[0;36mpreprocess_videos\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Loop over all files in the input directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /การเงินและธุรกรรม'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5cVDAWIgp7"
      },
      "source": [
        "# PyThaiNLP Get Started\n",
        "\n",
        "Code examples for basic functions in PyThaiNLP https://github.com/PyThaiNLP/pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HsfhZlwInqs"
      },
      "outputs": [],
      "source": [
        "# # pip install required modules\n",
        "# # uncomment if running from colab\n",
        "# # see list of modules in `requirements` and `extras`\n",
        "# # in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
        "\n",
        "#!pip install pythainlp\n",
        "#!pip install epitran"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install required modules"
      ],
      "metadata": {
        "id": "BaT_g8fV-7xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install epitran"
      ],
      "metadata": {
        "id": "E32blbWe_CLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR6Klwc-UAH"
      },
      "source": [
        "## Import PyThaiNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp"
      ],
      "metadata": {
        "id": "9IhnIXQJ-oDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7CkITTf-UAH"
      },
      "outputs": [],
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6gy4MLGIgp9"
      },
      "source": [
        "## Thai Characters\n",
        "\n",
        "PyThaiNLP provides some ready-to-use Thai character set (e.g. Thai consonants, vowels, tonemarks, symbols) as a string for convenience. There are also few utility functions to test if a string is in Thai or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvoeZg3Igp-"
      },
      "outputs": [],
      "source": [
        "pythainlp.thai_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPtK_FL-UAI"
      },
      "outputs": [],
      "source": [
        "len(pythainlp.thai_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPwx53A6IgqF"
      },
      "outputs": [],
      "source": [
        "pythainlp.thai_consonants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5-lZjsd-UAJ"
      },
      "outputs": [],
      "source": [
        "len(pythainlp.thai_consonants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UA7Hwy_IgqI"
      },
      "outputs": [],
      "source": [
        "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32vFPqeB-UAJ"
      },
      "source": [
        "## Checking if a string contains Thai character or not, or how many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3NvXqYFIgqK"
      },
      "outputs": [],
      "source": [
        "import pythainlp.util\n",
        "\n",
        "pythainlp.util.isthai(\"ก\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRzSQjugIgqM"
      },
      "outputs": [],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP5yfJebIgqP"
      },
      "outputs": [],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\", ignore_chars=\".()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFPOHyZIgqh"
      },
      "source": [
        "## Tokenization and Segmentation\n",
        "\n",
        "At sentence, word, and sub-word levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzDO2rIP-UAN"
      },
      "source": [
        "### Sentence\n",
        "\n",
        "Default sentence tokenizer is \"crfcut\". Tokenization engine can be chosen ussing `engine=` parameter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize"
      ],
      "metadata": {
        "id": "WDZtB1BND8m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SklPJ-DbIgqi"
      },
      "source": [
        "### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "text = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default (newmm):\")\n",
        "print(word_tokenize(text))\n",
        "print(\"\\nnewmm and keep_whitespace=False:\")\n",
        "print(word_tokenize(text, keep_whitespace=False))"
      ],
      "metadata": {
        "id": "Ej3Zio1eIWXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEbY-MGCIgqi"
      },
      "outputs": [],
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "text = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default (newmm):\")\n",
        "print(word_tokenize(text))\n",
        "print(\"\\nnewmm and keep_whitespace=False:\")\n",
        "print(word_tokenize(text, keep_whitespace=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5P_YygrIgqm"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI_Qz3k3Igqm"
      },
      "outputs": [],
      "source": [
        "from pythainlp import word_tokenize, Tokenizer\n",
        "\n",
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"newmm  :\", word_tokenize(text))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text, engine=\"longest\"))\n",
        "\n",
        "words = [\"กูเกิ้ลโครม\", \"กูเกิ้ลไดร์ฟ\"]\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIXUxXlTIgqo"
      },
      "source": [
        "Default word tokenizer use a word list from `pythainlp.corpus.common.thai_words()`.\n",
        "We can get that list, add/remove words, and create new tokenizer from the modified list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RblqNckGIgqp"
      },
      "outputs": [],
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "\n",
        "text = \"นิยายวิทยาศาสตร์ของไอแซค อสิมอฟ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"ไอแซค\")  # Isaac\n",
        "words.add(\"อสิมอฟ\")  # Asimov\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "\n",
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))"
      ],
      "metadata": {
        "id": "JnPHAqegLPPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tNJ7pxustyZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "words = ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ernbPKbDOq2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "IdPMtwGPqLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "\n",
        "def reorder_to_tsl(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Basic reordering to follow TSL rules\n",
        "    # Step 1: Move time words to the beginning\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # Join the reordered sentence back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# Example sentence in Thai\n",
        "sentence = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence to follow TSL grammar rules\n",
        "tsl_sentence = reorder_to_tsl(sentence)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "id": "Iay6ZOMRm5kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')\n",
        "\n",
        "    # Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(words, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Reorder: Nouns -> Others -> Verbs\n",
        "    reordered_sentence = nouns + others + verbs\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# เรียงคำนามก่อนคำกริยา\n",
        "reordered_sentence = reorder_noun_before_verb(sentence)\n",
        "print(\"ประโยคที่เรียงคำนามก่อนกริยา:\", reordered_sentence)"
      ],
      "metadata": {
        "id": "4MKtlueewfIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Reorder: Nouns -> Others -> Verbs\n",
        "    reordered_sentence = nouns + others + verbs\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence = reorder_to_tsl_and_noun_before_verb(sentence)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา:\", reordered_sentence)"
      ],
      "metadata": {
        "id": "1_YvPxmnyLuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "vG3y24f52B2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.augment.word2vec import Thai2fitAug\n",
        "\n",
        "# สร้าง instance ของ Thai2fitAug\n",
        "aug = Thai2fitAug()\n",
        "\n",
        "# ใช้ Thai2fitAug เพื่อทำการเปลี่ยนแปลงคำในประโยค\n",
        "sentence = \"เดินทาง\"\n",
        "augmented_sentences = aug.augment(sentence, n_sent=2, p=0.5)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "for variation in augmented_sentences:\n",
        "    print(variation)"
      ],
      "metadata": {
        "id": "rzjxwUEP8crO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ตัวอย่างฟังก์ชันการหาคำพื้นฐานที่สร้างเอง\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"เดินทาง\": \"เดิน\",\n",
        "        \"รับประทาน\": \"กิน\",\n",
        "        \"อาคาร\": \"บ้าน\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# การใช้งานฟังก์ชัน\n",
        "print(get_lemma(\"เดินทาง\"))  # Output: เดิน"
      ],
      "metadata": {
        "id": "fn0fYlPVAt74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fEsTNEt_Bchc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}